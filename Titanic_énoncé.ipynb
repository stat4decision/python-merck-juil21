{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Competition | Titanic Machine Learning from Disaster\n",
    "\n",
    ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    ">One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    ">In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    ">This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n",
    "\n",
    "From the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n",
    "\n",
    "\n",
    "### Goal for this Notebook:\n",
    "Show a simple example of an analysis of the Titanic disaster in Python using a full complement of PyData utilities. This is aimed for those looking to get into the field or those who are already in the field and looking to see an example of an analysis done with Python.\n",
    "\n",
    "#### This Notebook will show basic examples of: \n",
    "#### Data Handling\n",
    "*   Importing Data with Pandas\n",
    "*   Cleaning Data\n",
    "*   Exploring Data through Visualizations with Matplotlib\n",
    "\n",
    "#### Data Analysis\n",
    "*    Supervised Machine learning Techniques:\n",
    "    +   Logit Regression Model \n",
    "    +   Plotting results\n",
    "    +   Support Vector Machine (SVM) using 3 kernels\n",
    "    +   Basic Random Forest\n",
    "    +   Plotting results\n",
    "\n",
    "#### Valuation of the Analysis\n",
    "*   K-folds cross validation to valuate results locally\n",
    "*   Output the results from the IPython Notebook to Kaggle\n",
    "\n",
    "\n",
    "\n",
    "#### Required Libraries:\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [IPython](http://ipython.org/)\n",
    "* [Pandas](http://pandas.pydata.org/)\n",
    "* [SciKit-Learn](http://scikit-learn.org/stable/)\n",
    "* [SciPy](http://www.scipy.org/)\n",
    "* [StatsModels](http://statsmodels.sourceforge.net/)\n",
    "* [Patsy](http://patsy.readthedocs.org/en/latest/)\n",
    "* [Matplotlib](http://matplotlib.org/)\n",
    "\n",
    "***To run this notebook interactively, get it from my Github [here](https://github.com/agconti/kaggle-titanic). The competition's website is located on [Kaggle.com](http://www.kaggle.com/c/titanic-gettingStarted).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn import datasets, svm\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling\n",
    "#### Let's read our data in using pandas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an overview of our data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exploration du dataframe \n",
    "- Regarder les premières lignes du dataframe\n",
    "- Regarder un résumé du dataframe avec df.describe(include='all')\n",
    "- Combien y a t il de lignes ? \n",
    "- Y a t il des colonnes avec des valeurs manquantes ?\n",
    "- Quelles sont les variables numériques, catégorielles, d'identifiant ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection des colonnes\n",
    "* A votre avis, quelles colonnes peut-on supprimer pour la construction du dataset d'entrainement ? Pourquoi ?\n",
    "* Ecrire une fonction `filter_columns` qui prend en entrée le dataframe brut, et supprime les colonnes inutiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion des valeurs manquantes\n",
    "* La variable `age` contient des valeurs manquantes, comment proposez vous des les gérer ? \n",
    "* ecrire une fonction `manage_nan` qui prend en entrée le dataframe et applique la solution proposée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrire maintenant une fonction clean_data, qui applique à la suite les deux fonctions précédémment créées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez la fonction clean_data pour créer un nouveau dataframe 'df_cleaned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un peu de pandas pour jouer avec les données ...\n",
    "Effectuer des aggrégations pour regarder :\n",
    "* La moyenne de survie agrégée par classe, par sexe, par sexe + classe (avec un groupby)\n",
    "* La moyenne de survie croisée, avec en ligne les classes, en colonnes les différentes modalités de la colonne sex : 'M' et 'F' (avec un pivot_table)\n",
    "* Pour chacune des tables aggrégées, tracez les graphiques (bar) correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse graphique avec searborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracez les plots suivants. Que représentent-ils ? Que peut-on en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Sex',data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "sns.countplot(x=\"Pclass\", hue=\"Survived\", data=df_cleaned)\n",
    "plt.ylabel('# of persons')\n",
    "plt.title(\"Number of survivors by class\")\n",
    "plt.legend(loc=\"upper left\", title=\"Survived\")\n",
    "\n",
    "fig = plt.figure() \n",
    "sns.countplot(x=\"Sex\", hue=\"Survived\", data=df_cleaned)\n",
    "plt.ylabel('# of persons')\n",
    "plt.title(\"Number of survivors by Sex\")\n",
    "plt.legend(loc=\"upper right\", title=\"Survived\")\n",
    "\n",
    "fig = plt.figure() \n",
    "sns.countplot(x=\"Parch\", hue=\"Survived\", data=df_cleaned)\n",
    "plt.ylabel('# of persons')\n",
    "plt.title(\"Number of survivors by Parch\")\n",
    "plt.legend(loc=\"upper right\", title=\"Survived\")\n",
    "\n",
    "fig = plt.figure() \n",
    "sns.countplot(x=\"SibSp\", hue=\"Survived\", data=df_cleaned)\n",
    "plt.ylabel('# of persons')\n",
    "plt.title(\"Number of survivors by SibSp\")\n",
    "plt.legend(loc=\"upper right\", title=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df_cleaned, col='Sex')\n",
    "g.map(sns.distplot, 'Age',kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Fare',y='Age',data=df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(df.corr(), annot=True, cmap= 'coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Pclass',y='Age',data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x='Pclass',y='Age',data=df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des sets d'entraînement et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un premier modèle ultra simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\") \n",
    "feature_cols = ['Pclass', 'Parch']\n",
    "X = df[feature_cols]\n",
    "y = df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement d'une régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions et calcul de la précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions de la classe\n",
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de la précision\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison avec un modèle simpliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_simpliste = np.zeros(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred_simpliste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des différentes valeurs (vrais / faux positifs / négatifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "TP = confusion[1][1]\n",
    "TN = confusion[0][0]\n",
    "FP = confusion[0][1]\n",
    "FN = confusion[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vrais Positifs :', TP)\n",
    "print('Vrais Négatifs :', TN)\n",
    "print('Faux Positifs :', FP)\n",
    "print('Faux Négatifs :', FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La sensibilité\n",
    "\n",
    "Aussi appelée rappel ou taux de vrais positifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (TP / float(TP + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de faux positifs\n",
    "\n",
    "Ce sont les fausses alarmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (FP / float(FP + TN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de faux négatifs\n",
    "\n",
    "Les positifs ratés par le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (FN / float(TP + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La spécificité\n",
    "\n",
    "C'est le taux de vrais négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (TN / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédire des probabilités d'appartenance à une classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des probabilités de survie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_prob);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_prob > 0.5).astype(int)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenter la sensibilité en abaissant le seuil de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "y_pred_class.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = np.where(y_pred_prob > 0.3, 1, 0)\n",
    "y_pred_class.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ancienne matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nouvelle matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensibilité plus basse\n",
    "print (TP / float(TP + FN))\n",
    "print (63 / float(63 + 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spécificité plus haute\n",
    "print( TN / float(TN + FP))\n",
    "print (72 / float(72 + 56))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de la courbe de ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, classification_report\n",
    "def plot_auc(y_test, y_pred_prob):\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(fpr, tpr, label='AUC: %0.3f' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('1-Specificite')\n",
    "    plt.ylabel('Sensibilite')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un peu de feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice : Remplacer \"Sex\" par une valeur booléenne avec du dummy encoding dans df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexe = pd.get_dummies(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexe.drop(['Sex_female'], axis=1, inplace=True)\n",
    "df.drop(['Sex'], axis=1, inplace=True)\n",
    "df = pd.concat([df, sexe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice :\n",
    "* écrire une fonction `category_feature_engineer` qui construit les variables dummifiées pour toutes les colonnes catégorielles\n",
    "* créer une fonction `preprocess_data` qui enchaine les traitements de clean_data et category_feature_engineer\n",
    "* Réécrire toutes les fonctions de pré-traitement dans la cellule ci dessous\n",
    "* appliquer la fonction `preprocess_data` sur le dataframe initial pour créer un dataframe `preprocessed_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/titanic/train.csv')\n",
    "preprocessed_df = preprocess_data(df)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouvelle régression logistique avec toutes les features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les données et appliquer la fonction de preprocessing pour créer les jeux de données X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créee une régression logistique l'entrainer sur ces données préprocessées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des coefficients\n",
    "On peut regarder le poids de chaque coefficient de la régression, ce qui donne une mesure de l'importance de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train.columns, logreg.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_summary(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un autre modèle : la random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec les paramètres par défaut\n",
    "* appliquer le même pipeline de prétraitement des données, mais avec la forêt aléatoire en classifieur.\n",
    "* Quel score obtient-t-on en auc ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En changeant les paramètres\n",
    "* Essayer d'autres paramètres pour la forêt aléatoire \n",
    "* Arrivez-vous à augmenter le score ? Avec quels paramètres ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation des hyper-paramètres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Construire un gridsearch pour l'optimsiation des hyper paramètres de la forêt aléatoire (appeler le gridsearch `clf`)\n",
    "* Quel score obtenez vous ? Quelle courbe ROC ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.best_estimator_ contient le meilleur modèle trouvé par le gridsearch. \n",
    "# clf.best_estimator_.feature_importances_ contient un score mesurant à quel point la feature a été \n",
    "# utilisée par le random forest pour faire des prédictions.\n",
    "list(zip(X_train.columns, clf.best_estimator_.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice:\n",
    "* Essayer avec d'autres valeurs pour la grille de paramètres\n",
    "* Essayer avec un gridsearch sur un autre modèle de votre choix (cf documentation sklearn)\n",
    "* Quel score (auc) arrivez vous à obtenir ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tout mettre ensemble : les pipelines !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les pipelines sont des objets permettant d'enchainer séquentiellement des étapes de traitement de données.\n",
    "# Un pipeline sklearn contient un nombre illimité de transformers, et un unique modèle prédictif, à la fin du pipeline.\n",
    "# Les transformers sont des classes qui doivent hériter de la classe TransformerMixin de sklearn, \n",
    "# et implémenter les méthodes 'fit' et 'transform'\n",
    "\n",
    "# On créé un transformer qui applique le cleaning et la création de nouvelles variables \n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fill_na_with='median'):\n",
    "        # Le processor sera initialisé avec des paramètres (ici on a 1 paramètre)\n",
    "        # Ces paramètres pourront être optimisés par la suite avec un gridsearch sur le pipeline.\n",
    "        self.fill_na_with = fill_na_with\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        X_preprocessed = preprocess_data(X, self.fill_na_with)\n",
    "        return X_preprocessed\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "# Note : ici on appelle la fonction 'preprocess_data' que l'on a construite plus haut. \n",
    "# Pour faire les choses proprement, il faudrait intégrer la fonction 'preprocess_data'\n",
    "# dans les méthodes de la classe Preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/titanic/train.csv')\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "# On applique le preprocessor sur le dataframe initial !\n",
    "X_transformed = preprocessor.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créé un pipeline avec notre class de preprocessing et un modèle randomforest\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('preprocess', Preprocessor()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le pipeline s'entraine sur le jeu d'entrainement \n",
    "df = pd.read_csv('data/titanic/train.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df.Survived)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "y_pred_prob = pipe.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut entrainer un grid search sur le pipeline, en jouant sur tous les paramètres :paramètre du modèle\n",
    "# mais aussi paramètre des étapes de prétraitement. Pour cela, on fait préfixer le nom du paramètre par\n",
    "# '{nom de l'etape}__'. Par exemple, l'étape 'preprocess' possède un paramètre 'fill_na_with' donc on écrit:\n",
    "# preprocess__fill_na_with pour le paramètre de cette étape.\n",
    "pipeline_params = {\n",
    "    'preprocess__fill_na_with': ['mean', 'median'],\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'rf__criterion': ['gini', 'entropy'],\n",
    "    'rf__max_depth': [None, 10, 50]\n",
    "}\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('preprocess', Preprocessor()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "grid = GridSearchCV(pipe, pipeline_params)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "y_pred_prob = grid.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_test, y_pred_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
